{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, math, threading\n",
    "import sqlite3, os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from queue import Queue\n",
    "from selenium_tools import get_chrome, find_element, get_soup\n",
    "from fake_useragent import UserAgent\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.google.com/maps/\"\n",
    "datas = pd.read_csv(\"Taipei_to_hots_copy1.csv\", encoding = \"utf-8-sig\")\n",
    "datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_comments(name):\n",
    "    while True:\n",
    "        chrome = get_chrome(url, hide = True)\n",
    "        if \"yZqPAf\" in str(chrome.page_source):\n",
    "            chrome = get_chrome(url, hide = True)\n",
    "        else:\n",
    "            break\n",
    "    chrome.refresh()\n",
    "    #網頁最大化\n",
    "    chrome.maximize_window()\n",
    "    xpath1 = \"/html/body/div[1]/div[3]/div[8]/div[3]/div[1]/div[1]/div/div[2]/form/input\"\n",
    "    element = find_element(chrome, xpath = xpath1)\n",
    "    element.clear()\n",
    "    element.click()\n",
    "    element.send_keys(name + \"\\n\")\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(chrome.page_source, \"html.parser\")\n",
    "    if \"RZ66Rb\" not in str(chrome.page_source):\n",
    "        url_new = soup.find(\"a\", class_ = \"hfpxzc\").get(\"href\")            \n",
    "        chrome.quit()\n",
    "        chrome = get_chrome(url_new, hide = False)\n",
    "        time.sleep(0.5)\n",
    "        #網頁最大化\n",
    "        chrome.maximize_window()\n",
    "        chrome.refresh()    \n",
    "    elif \"RZ66Rb\" in str(chrome.page_source):\n",
    "        time.sleep(0.5)\n",
    "    soup = BeautifulSoup(chrome.page_source, \"html.parser\") \n",
    "    com_list = [name]\n",
    "    if \"Gpq6kf fontTitleSmall\" in str(chrome.page_source):\n",
    "        try:\n",
    "            try:\n",
    "                xpath = \"/html/body/div[1]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div/div/button[2]\"\n",
    "                element = chrome.find_element(By.XPATH, xpath)\n",
    "                element.click()                \n",
    "            except Exception as e:\n",
    "                xpath = \"/html/body/div[1]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div/div/button[3]\"\n",
    "                element = chrome.find_element(By.XPATH, xpath)\n",
    "                element.click()            \n",
    "        except Exception as e:\n",
    "            try:\n",
    "                xpath = \"/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div/div/button[2]\"\n",
    "                element = chrome.find_element(By.XPATH, xpath)\n",
    "                element.click()                \n",
    "            except Exception as e:\n",
    "                xpath = \"/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div/div/button[3]\"\n",
    "                element = chrome.find_element(By.XPATH, xpath)\n",
    "                element.click()      \n",
    "        time.sleep(0.5)\n",
    "        try:\n",
    "            try: \n",
    "                comment_roller = \"/html/body/div[1]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]\"                \n",
    "                pane = chrome.find_element(By.XPATH, comment_roller)\n",
    "                chrome.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", pane)\n",
    "            except Exception as e:\n",
    "                comment_roller = \"/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[4]\"\n",
    "                pane = chrome.find_element(By.XPATH, comment_roller)\n",
    "                chrome.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", pane)                     \n",
    "        except Exception as e:\n",
    "            try: \n",
    "                comment_roller = \"/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]\"                \n",
    "                pane = chrome.find_element(By.XPATH, comment_roller)\n",
    "                chrome.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", pane)\n",
    "            except Exception as e:\n",
    "                comment_roller = \"/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[4]\"\n",
    "                pane = chrome.find_element(By.XPATH, comment_roller)\n",
    "                chrome.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", pane)   \n",
    "        time.sleep(0.5)\n",
    "        try:\n",
    "            try: \n",
    "                comment_roller = \"/html/body/div[1]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]\"                \n",
    "                pane1 = chrome.find_element(By.XPATH, comment_roller)\n",
    "                chrome.execute_script(\"arguments[0].scrollTop = 0\", pane1)\n",
    "            except Exception as e:\n",
    "                comment_roller = \"/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[4]\"\n",
    "                pane1 = chrome.find_element(By.XPATH, comment_roller)\n",
    "                chrome.execute_script(\"arguments[0].scrollTop = 0\", pane1)                     \n",
    "        except Exception as e:\n",
    "            try: \n",
    "                comment_roller = \"/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]\"                \n",
    "                pane1 = chrome.find_element(By.XPATH, comment_roller)\n",
    "                chrome.execute_script(\"arguments[0].scrollTop = 0\", pane1)\n",
    "            except Exception as e:\n",
    "                comment_roller = \"/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[4]\"\n",
    "                pane1 = chrome.find_element(By.XPATH, comment_roller)\n",
    "                chrome.execute_script(\"arguments[0].scrollTop = 0\", pane1)  \n",
    "        time.sleep(0.5)\n",
    "        soup2 = BeautifulSoup(chrome.page_source, \"html.parser\")\n",
    "        comments = soup2.find_all(\"div\", class_ = \"jftiEf fontBodyMedium\")\n",
    "        time.sleep(2)\n",
    "        for i in comments:\n",
    "            if \"w8nwRe\" in str(i): \n",
    "                element = chrome.find_element(By.CLASS_NAME, 'w8nwRe')\n",
    "                element.click()\n",
    "        time.sleep(0.5)\n",
    "        try:\n",
    "            try: \n",
    "                comment_roller = \"/html/body/div[1]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]\"                \n",
    "                pane1 = chrome.find_element(By.XPATH, comment_roller)\n",
    "                chrome.execute_script(\"arguments[0].scrollTop = 0\", pane1)\n",
    "            except Exception as e:\n",
    "                comment_roller = \"/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[4]\"\n",
    "                pane1 = chrome.find_element(By.XPATH, comment_roller)\n",
    "                chrome.execute_script(\"arguments[0].scrollTop = 0\", pane1)                     \n",
    "        except Exception as e:\n",
    "            try: \n",
    "                comment_roller = \"/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]\"                \n",
    "                pane1 = chrome.find_element(By.XPATH, comment_roller)\n",
    "                chrome.execute_script(\"arguments[0].scrollTop = 0\", pane1)\n",
    "            except Exception as e:\n",
    "                comment_roller = \"/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[4]\"\n",
    "                pane1 = chrome.find_element(By.XPATH, comment_roller)\n",
    "                chrome.execute_script(\"arguments[0].scrollTop = 0\", pane1)\n",
    "        time.sleep(0.5)\n",
    "        soup2 = BeautifulSoup(chrome.page_source, \"html.parser\")\n",
    "        comments = soup2.find_all(\"div\", class_ = \"jftiEf fontBodyMedium\")\n",
    "        chrome.quit()\n",
    "        print(name + \"SuccessFul!\")\n",
    "        for com in comments:\n",
    "            if \"wiI7pd\" in str(com):\n",
    "                com_list.extend([\"姓名: \" + com.find(class_ = \"d4r55\").text, \"評論: \" + com.find(\"span\", class_ = \"wiI7pd\").text])\n",
    "            else:\n",
    "                com_list.extend([\"姓名: \" + com.find(class_ = \"d4r55\").text, \"評論: \" + \"很讚\"])\n",
    "    else:\n",
    "        com_list = [name, \"暫無評價\"]      \n",
    "    return com_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        chrome = get_chrome(url, hide = False)\n",
    "        if \"yZqPAf\" in str(chrome.page_source):\n",
    "            chrome = get_chrome(url, hide = False)\n",
    "        else:\n",
    "            break\n",
    "    chrome.refresh()\n",
    "    #網頁最大化\n",
    "    chrome.maximize_window()\n",
    "    xpath1 = \"/html/body/div[1]/div[3]/div[8]/div[3]/div[1]/div[1]/div/div[2]/form/input\"\n",
    "    element = find_element(chrome, xpath = xpath1)\n",
    "    element.clear()\n",
    "    element.click()\n",
    "    element.send_keys(datas[\"shopName\"][149] + \"\\n\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 有選項"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if \"Nv2PK Q2HXcd THOPZb\" in str(chrome.page_source):\n",
    "        soup = BeautifulSoup(chrome.page_source, \"html.parser\")\n",
    "        num = -1\n",
    "        for number in soup.find_all(\"div\", class_ = \"Nv2PK Q2HXcd THOPZb\"):\n",
    "            number_text = eval(number.find(\"span\", class_ = \"UY7F9\").text)\n",
    "            if number_text > num:\n",
    "                num = number_text\n",
    "                url_new = number.find(\"a\", class_ = \"hfpxzc\").get(\"href\")\n",
    "        chrome.quit()\n",
    "        chrome = get_chrome(url_new, hide = False)\n",
    "        time.sleep(0.5)\n",
    "        #網頁最大化\n",
    "        chrome.maximize_window()\n",
    "        chrome.refresh()    \n",
    "    elif \"RZ66Rb\" in str(chrome.page_source):\n",
    "        time.sleep(0.5)\n",
    "    try:\n",
    "        xpath = \"/html/body/div[1]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div/div/button[2]\"\n",
    "        element = chrome.find_element(By.XPATH, xpath)\n",
    "        element.click()\n",
    "    except Exception as e:       \n",
    "        xpath = \"/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[3]/div/div/button[2]\"\n",
    "        element = chrome.find_element(By.XPATH, xpath)\n",
    "        element.click()\n",
    "    time.sleep(0.5)\n",
    "    try:        \n",
    "        comment_roller = \"/html/body/div[1]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]\"    \n",
    "        pane = chrome.find_element(By.XPATH, comment_roller)\n",
    "        chrome.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", pane)\n",
    "    except Exception as e:\n",
    "        comment_roller = \"/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]\"    \n",
    "        pane = chrome.find_element(By.XPATH, comment_roller)\n",
    "        chrome.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", pane)   \n",
    "    time.sleep(0.5)\n",
    "    try:        \n",
    "        comment_roller = \"/html/body/div[1]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]\"    \n",
    "        pane1 = chrome.find_element(By.XPATH, comment_roller)\n",
    "        chrome.execute_script(\"arguments[0].scrollTop = 0\", pane1)\n",
    "    except Exception as e:\n",
    "        comment_roller = \"/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]\"    \n",
    "        pane1 = chrome.find_element(By.XPATH, comment_roller)\n",
    "        chrome.execute_script(\"arguments[0].scrollTop = 0\", pane1) \n",
    "    time.sleep(0.5)\n",
    "    soup2 = BeautifulSoup(chrome.page_source, \"html.parser\")\n",
    "    comments = soup2.find_all(\"div\", class_ = \"jftiEf fontBodyMedium\")\n",
    "    time.sleep(2)\n",
    "    for i in comments:\n",
    "        if \"w8nwRe\" in str(i): \n",
    "            element = chrome.find_element(By.CLASS_NAME, 'w8nwRe')\n",
    "            element.click()\n",
    "    time.sleep(0.5)\n",
    "    try:        \n",
    "        comment_roller = \"/html/body/div[1]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]\"    \n",
    "        pane1 = chrome.find_element(By.XPATH, comment_roller)\n",
    "        chrome.execute_script(\"arguments[0].scrollTop = 0\", pane1)\n",
    "    except Exception as e:\n",
    "        comment_roller = \"/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]\"    \n",
    "        pane1 = chrome.find_element(By.XPATH, comment_roller)\n",
    "        chrome.execute_script(\"arguments[0].scrollTop = 0\", pane1) \n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    soup2 = BeautifulSoup(chrome.page_source, \"html.parser\")\n",
    "    comments = soup2.find_all(\"div\", class_ = \"jftiEf fontBodyMedium\")\n",
    "    print(len(comments))\n",
    "    chrome.quit()\n",
    "#     print(comments)\n",
    "    com_list = [name]\n",
    "    for com in comments:\n",
    "        if \"wiI7pd\" in str(com):\n",
    "            com_list.extend([\"姓名: \" + com.find(class_ = \"d4r55\").text, \"評論: \" + com.find(\"span\", class_ = \"wiI7pd\").text])\n",
    "        else:\n",
    "            com_list.extend([\"姓名: \" + com.find(class_ = \"d4r55\").text, \"評論: \" + \"很讚\"])\n",
    "    print(pd.DataFrame(com_list))    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    all_comments = []\n",
    "    chrome = None\n",
    "    for name in datas[\"shopName\"][148:]:        \n",
    "        while True:\n",
    "            try:\n",
    "                com_list = for_comments(name)\n",
    "                if com_list[2:] == []:\n",
    "                    com_list = for_comments(name)\n",
    "            except Exception as e:\n",
    "                if chrome != None:\n",
    "                    chrome.quit()\n",
    "                com_list = for_comments(name)\n",
    "                if com_list[2:] == []:\n",
    "                    com_list = for_comments(name)\n",
    "            else:\n",
    "                break\n",
    "        all_comments.append(com_list)\n",
    "        time.sleep(2)\n",
    "    print(all_comments)          \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(all_comments)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"tapei_comments_1.csv\", encoding = \"UTF-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
